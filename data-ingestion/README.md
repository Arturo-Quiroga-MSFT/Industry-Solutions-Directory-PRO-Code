# Data Ingestion - Industry Solutions Directory

**Solution Owner:** Arturo Quiroga  
**Role:** Principal Industry Solutions Architect, Microsoft  
**Purpose:** Data ingestion pipeline to index Microsoft partner solutions from the Industry Solutions Directory into Azure AI Search for AI-powered search

---

## Directory Structure

```
data-ingestion/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ verify_index.py                # Verify indexed data
â”œâ”€â”€ partner_analysis.py            # Analyze partner statistics
â”œâ”€â”€ API_INVESTIGATION.md           # API discovery documentation
â”œâ”€â”€ RELEVANCE_SCORE.md             # Search relevance documentation
â”‚
â”œâ”€â”€ integrated-vectorization/      # â­ CURRENT PRODUCTION APPROACH
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_create_vectorizer.py
â”‚   â”œâ”€â”€ 02_create_index.py
â”‚   â”œâ”€â”€ 03_ingest_data.py
â”‚   â””â”€â”€ 04_test_search.py
â”‚
â”œâ”€â”€ archive/                       # Historical/archived files
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ old-manual-ingestion/      # Old client-side vectorization approach
â”‚   â”œâ”€â”€ mcs-compatibility/         # Old MCS compatibility attempts
â”‚   â””â”€â”€ logs-and-output/           # Historical logs
â”‚
â””â”€â”€ reports/                       # Current reports and analysis
    â”œâ”€â”€ README.md
    â”œâ”€â”€ current_index_verification.txt
    â”œâ”€â”€ full_ingestion_summary.md
    â””â”€â”€ solutions-found.json
```

---

## ðŸŽ¯ Quick Start (Current Approach)

The **production approach** uses Azure AI Search **integrated vectorization** (v2.8+):

```bash
cd integrated-vectorization

# 1. Create vectorizer (one-time)
python 01_create_vectorizer.py

# 2. Create index with integrated vectorization (one-time)
python 02_create_index.py

# 3. Ingest data
python 03_ingest_data.py

# 4. Test search
python 04_test_search.py
```

**Benefits of integrated-vectorization:**
- âœ… Automatic query vectorization (no client-side embedding needed)
- âœ… Direct REST API (no Python SDK dependency issues)
- âœ… Simpler code and maintenance
- âœ… Better performance

See [`integrated-vectorization/README.md`](integrated-vectorization/README.md) for details.

---

## Current Index Statistics

- **Index Name:** `partner-solutions-integrated`
- **Documents:** 535 documents
- **Unique Solutions:** 376 partner solutions
- **Industries:** 10+ major industries covered
- **Vectorization:** Azure OpenAI text-embedding-3-large (3072 dimensions)

Run `python verify_index.py` to see current statistics.

---

## Historical Context

### Old Manual Approach (Archived)

The original implementation (archived in `archive/old-manual-ingestion/`) used:
- Client-side embedding generation with Python SDK
- Manual chunking and vectorization
- azure-search-documents 11.6.0 (had compatibility issues)

**Why it was replaced:**
- Python SDK incompatibility with integrated vectorization
- More complex code requiring client-side embedding
- Maintenance burden for embedding generation

See [`archive/README.md`](archive/README.md) for historical context.

---

## Web Scraping Implementation

The script uses a **multi-tiered approach** to retrieve solution data:

### 1. API-First Approach
Attempts to discover and use the website's backend API endpoints:
- `/api/solutions`
- `/api/v1/solutions`
- `/api/data/solutions`

This is the preferred method as it provides structured data directly from the source.

### 2. HTML Scraping Fallback
If API discovery fails, the script falls back to scraping the rendered HTML page:
- Parses solution cards from `/browse` page
- Extracts: solution names, descriptions, partner info, URLs
- Uses BeautifulSoup4 for HTML parsing

### 3. Sample Data Fallback
If both methods fail (e.g., during development/testing), the script uses 3 sample solutions to ensure the pipeline can be tested end-to-end.

## Data Structure

Each solution is structured as:
```python
{
    "solution_name": str,          # Name of the solution
    "partner_name": str,            # Microsoft partner name
    "description": str,             # Solution description
    "industries": List[str],        # Target industries
    "technologies": List[str],      # Technologies used
    "solution_url": str,            # Link to solution details
    "full_content": str             # Full solution content
}
```

## How It Works

1. **Scrape**: Retrieves solution data from the website
2. **Chunk**: Breaks long content into 1000-character chunks
3. **Embed**: Generates vector embeddings using `text-embedding-3-large`
4. **Index**: Uploads to Azure AI Search with:
   - Vector search (semantic similarity)
   - Keyword search (BM25)
   - Filters (industry, technology, partner)
   - Facets for filtering UI

## Usage

```bash
# Set environment variables
export AZURE_SEARCH_ENDPOINT="https://your-search.search.windows.net"
export AZURE_OPENAI_ENDPOINT="https://your-openai.openai.azure.com/"
export AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME="text-embedding-3-large"

# Run the script
python data-ingestion/ingest_data.py
```

## Dependencies

- `requests` - HTTP client for API calls
- `beautifulsoup4` - HTML parsing
- `azure-search-documents` - Azure AI Search SDK
- `azure-identity` - Azure authentication

Install all dependencies:
```bash
pip install -r backend/requirements.txt
```

## Authentication

The script uses **Azure CLI authentication** (DefaultAzureCredential):
- Ensure you're logged in: `az login`
- RBAC permissions required:
  - Search Index Data Contributor
  - Search Service Contributor
  - Cognitive Services OpenAI User

## Customization

To adapt the scraper for the specific website structure:

1. **Inspect the website**: Open browser DevTools â†’ Network tab
2. **Find API calls**: Look for XHR/Fetch requests to `/api/*` endpoints
3. **Update `api_endpoints` list** in `scrape_solutions()` method
4. **Adjust HTML selectors** in `_scrape_html_page()` if needed

### Example: Custom API Parsing

```python
def _parse_api_solutions(self, data: List[Dict]) -> List[Dict[str, Any]]:
    solutions = []
    for item in data:
        solution = {
            "solution_name": item['name'],  # Adjust field names
            "partner_name": item['partner'],
            "industries": item['industries'].split(','),  # Handle format
            # ... customize as needed
        }
        solutions.append(solution)
    return solutions
```

## Troubleshooting

### No solutions found
- Check if website structure changed
- Verify API endpoints are correct
- Enable debug logging: `logging.basicConfig(level=logging.DEBUG)`

### Authentication errors
- Run `az login` to authenticate
- Verify RBAC role assignments
- Check service principal permissions in production

### Embedding failures
- Verify OpenAI deployment name matches
- Check quota/rate limits on OpenAI service
- Ensure embedding model is `text-embedding-3-large`

## Next Steps

Once data is indexed, you can:
1. Test search queries via Azure portal
2. Start the FastAPI backend to use the chat API
3. Integrate the chat widget into the website
